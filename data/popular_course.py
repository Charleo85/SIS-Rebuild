from pyspark import SparkContext

sc = SparkContext("spark://spark-master:7077", "PopularCourse")

# each worker loads a piece of the data file
data = sc.textFile("/tmp/data/inputs/course1.in", 2)

# step 1: read in the data file and generate a map
# key = user_id, value = course_id that they are viewing
pairs = data.map(lambda line: line.split(","))

# step 2: group all courses that a user has viewed into a list
# key = user_id, value = list of courses they viewed
view_list = pairs.groupByKey()

# step 3: extract all co-views generated by a particular user
# key = user_id, value = co-view of two courses (c1, c2)
def extract_pair(pair):
    ret = []
    arr = pair[1]
    for x in arr:
        for y in arr:
            if x < y: ret.append( (pair[0], (x, y)) )
    return ret

coview = view_list.flatMap(lambda pair: extract_pair(pair))
clean_coview = coview.distinct()

# step 4: discard the user info, and count each co-view as 1
# key = co-view of two courses, value = 1 (one occurrence)
occurrence = clean_coview.map(lambda pair: (pair[1], 1))

# step 5: count the occurrence for each co-view
# key = co-view of two courses, value = occurrence count
count = occurrence.reduceByKey(lambda x,y: x+y)

# step 6: filter our co-views with occurrence less than 3
# key = co-view of two courses, value = occurrence (>= 3)
filter_count = count.filter(lambda pair: pair[1] >= 3)

# prints the result to stdout
output = filter_count.collect()
for p, count in output:
    print("co-view (%s, %s); count %d" % (p[0], p[1] ,count))
print("Popular courses done")

sc.stop()
